# ğŸ¦ Flappy AI

**Train a neural network to play Flappy Bird â€” entirely in your browser.**

A browser-first reinforcement learning playground where you can watch, tune, and train a DQN agent in real-time. No server-side ML required â€” everything runs on your CPU via JavaScript.

ğŸ® **Live Demo:** [vibegames.it](https://vibegames.it)

---

## âœ¨ Features

- **Real-time DQN Training** â€” Watch the agent learn to navigate pipes with a custom neural network implementation (no TensorFlow.js dependency)
- **Live Hyperparameter Tuning** â€” Adjust epsilon, learning rate, gamma, and more while training
- **Neural Network Visualization** â€” See activations flow through the network in real-time (click to open detailed view)
- **Multiple Training Modes:**
  - ğŸ‹ï¸ **Training Mode** â€” Agent learns with epsilon-greedy exploration
  - âš¡ **Fast Training** â€” Skip rendering for 10x+ speedup with DQN workflow visualization
  - ğŸ¯ **Evaluation Mode** â€” Greedy policy (Îµ=0) for leaderboard runs
  - ğŸ® **Manual Play** â€” Take control anytime (your plays contribute to the replay buffer!)
- **Speed Control** â€” Run training from 0.25x to 10x speed
- **Metrics Dashboard** â€” Track rewards, episode lengths, Q-values, and training progress
- **Leaderboard** â€” Compete for the highest pipe count

---

## ğŸ—ï¸ Project Structure

```
flappy_ai/
â”œâ”€â”€ web_client/           # ğŸŒŸ Main browser app (Vue 3 + TypeScript)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/   # Vue components (GameCanvas, ControlPanel, etc.)
â”‚   â”‚   â”œâ”€â”€ game/         # Game engine, renderer, physics
â”‚   â”‚   â”œâ”€â”€ rl/           # DQN agent, neural network, replay buffer
â”‚   â”‚   â”œâ”€â”€ services/     # API client for leaderboard
â”‚   â”‚   â””â”€â”€ styles/       # CSS
â”‚   â””â”€â”€ public/
â”‚       â””â”€â”€ assets/       # Sprites and audio
â”‚
â”œâ”€â”€ FlapPyBird/           # ğŸ“š Original Python implementation (reference/deprecated)
â”‚   â”œâ”€â”€ rl/               # Python DQN implementation
â”‚   â”œâ”€â”€ src/              # Python game engine
â”‚   â””â”€â”€ checkpoints/      # Trained model weights
â”‚
â”œâ”€â”€ deploy.sh             # Deployment script for Hetzner/VPS
â”œâ”€â”€ docker-compose.yml    # Docker configuration
â””â”€â”€ Caddyfile             # Caddy web server config
```

---

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+ and npm

### Local Development

```bash
# Clone the repository
git clone https://github.com/giosilvi/flappy_ai.git
cd flappy_ai

# Install dependencies
cd web_client
npm install

# Start development server
npm run dev
```

Open [http://localhost:5173](http://localhost:5173) in your browser.

### Build for Production

```bash
cd web_client
npm run build
```

The built files will be in `web_client/dist/`.

---

## ğŸ§  How It Works

### DQN (Deep Q-Network)

The agent uses a Deep Q-Network to learn which action (flap or don't flap) maximizes future rewards:

1. **Observation Space** (6 inputs):
   - Bird's vertical position and velocity
   - Distance to next pipe
   - Pipe gap position
   - Distance to second pipe
   - Second pipe gap position

2. **Neural Network Architecture**:
   - Input layer: 6 neurons
   - Hidden layers: 2 Ã— 64 neurons (ReLU activation)
   - Output layer: 2 neurons (Q-values for each action)

3. **Training Process**:
   - Agent takes actions using Îµ-greedy policy
   - Experiences stored in replay buffer
   - Network trained on random mini-batches
   - Target network updated periodically for stability

### Reward Structure

| Event | Reward |
|-------|--------|
| Pass a pipe | +1.0 |
| Each step alive | -0.001 |
| Death (collision) | -1.0 |

---

## âš™ï¸ Configuration

### Hyperparameters (adjustable during training)

| Parameter | Default | Description |
|-----------|---------|-------------|
| Epsilon (Îµ) | 1.0 â†’ 0.01 | Exploration rate (decays automatically) |
| Learning Rate | 0.001 | Neural network learning rate |
| Gamma (Î³) | 0.99 | Discount factor for future rewards |
| Batch Size | 64 | Samples per training step |
| Buffer Size | 50,000 | Replay buffer capacity |

### Speed Settings

- **0.25x - 1x**: Slow motion for observation
- **2x - 5x**: Accelerated training with rendering
- **10x**: Fast training (no rendering)

---

## ğŸŒ Deployment

### Deploy to a VPS (Hetzner, DigitalOcean, etc.)

```bash
# SSH into your server
ssh root@your-server-ip

# Clone the repository
git clone https://github.com/giosilvi/flappy_ai.git
cd flappy_ai

# Run the deploy script with your domain
chmod +x deploy.sh
./deploy.sh yourdomain.com
```

The deploy script will:
1. Install Node.js 20.x
2. Build the Vue app
3. Set up Caddy with automatic HTTPS
4. Start the containerized web server

### Requirements

- Ubuntu 22.04+ (or similar)
- Docker and Docker Compose
- Domain pointing to your server's IP

---

## ğŸ› ï¸ Tech Stack

### Frontend (`web_client/`)
- **Vue 3** (Option API) â€” Reactive UI framework
- **TypeScript** â€” Type-safe JavaScript
- **Vite** â€” Fast build tool and dev server
- **Canvas API** â€” Game rendering
- **Web Workers** â€” Background training (fast mode)
- **Custom Neural Network** â€” Pure JS/TS implementation (no external ML libraries)

### Infrastructure
- **Caddy** â€” Web server with automatic HTTPS
- **Docker** â€” Containerization
- **Hetzner Cloud** â€” Hosting

### Python Reference (`FlapPyBird/`)
- **PyTorch** â€” Original DQN training
- **Pygame** â€” Original game engine

---

## ğŸ“Š Training Tips

1. **Start with high epsilon** â€” Let the agent explore randomly at first
2. **Watch the Q-values** â€” They should stabilize as training progresses
3. **Use fast mode** â€” Training is ~10x faster without rendering
4. **Be patient** â€” Good performance typically emerges after 10,000+ episodes
5. **Try manual play** â€” Your gameplay adds to the replay buffer!

---

## ğŸ¯ Roadmap

- [x] Browser-based DQN training
- [x] Real-time neural network visualization
- [x] Hyperparameter tuning UI
- [x] Fast training mode with Web Workers
- [x] Leaderboard system
- [ ] Model save/load to browser storage
- [ ] Champion model showcase on landing page
- [ ] Mobile-optimized controls
- [ ] Additional RL algorithms (PPO, A2C)

---

## ğŸ“œ License

This project is open source under the MIT License.

---

## ğŸ™ Acknowledgments

- Original Flappy Bird game by Dong Nguyen
- [FlapPyBird](https://github.com/sourabhv/FlapPyBird) â€” Python implementation reference
- Sprites and sounds from the classic Flappy Bird

---

**Built with â¤ï¸ for the RL community**
