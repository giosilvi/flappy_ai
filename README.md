# ğŸ¦ Flappy AI

**Train a neural network to play Flappy Bird â€” entirely in your browser.**

A browser-first reinforcement learning playground where you can watch, tune, and train a DQN agent in real-time. No server-side ML required â€” everything runs on your CPU via JavaScript.

ğŸ® **Live Demo:** [vibegames.it](https://vibegames.it)

---

## âœ¨ Features

- **Real-time DQN Training** â€” Watch the agent learn to navigate pipes with a custom neural network implementation (no TensorFlow.js dependency)
- **Live Hyperparameter Tuning** â€” Adjust epsilon, learning rate, and reward shaping while training
- **Neural Network Visualization** â€” See activations flow through the network in real-time (click to open detailed view with ReLU activation display)
- **Multiple Training Modes:**
  - ğŸ‹ï¸ **Training Mode** â€” Agent learns with epsilon-greedy exploration
  - âš¡ **Fast Training** â€” Skip rendering for maximum CPU utilization
  - ğŸ¯ **Evaluation Mode** â€” Greedy policy (Îµ=0) for leaderboard runs
  - ğŸ® **Manual Play** â€” Take control anytime (your plays contribute to the replay buffer!)
- **Metrics Dashboard** â€” Track rewards, episode lengths, Q-values, loss, and training progress
- **Checkpoint System** â€” Save and load training checkpoints
- **Leaderboard** â€” Compete for the highest pipe count

---

## ğŸ—ï¸ Project Structure

```
flappy_ai/
â”œâ”€â”€ web_client/           # ğŸŒŸ Main browser app (Vue 3 + TypeScript)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/   # Vue components (GameCanvas, ControlPanel, etc.)
â”‚   â”‚   â”œâ”€â”€ game/         # Game engine, renderer, physics
â”‚   â”‚   â”œâ”€â”€ rl/           # DQN agent, neural network, replay buffer
â”‚   â”‚   â”œâ”€â”€ services/     # API client for leaderboard
â”‚   â”‚   â””â”€â”€ styles/       # CSS
â”‚   â””â”€â”€ public/
â”‚       â””â”€â”€ assets/       # Sprites and audio
â”‚
â”œâ”€â”€ FlapPyBird/           # ğŸ“š Original Python implementation (reference/deprecated)
â”‚   â”œâ”€â”€ rl/               # Python DQN implementation
â”‚   â”œâ”€â”€ src/              # Python game engine
â”‚   â””â”€â”€ checkpoints/      # Trained model weights
â”‚
â”œâ”€â”€ deploy.sh             # Deployment script for Hetzner/VPS
â”œâ”€â”€ docker-compose.yml    # Docker configuration
â””â”€â”€ Caddyfile             # Caddy web server config
```

---

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+ and npm

### Local Development

```bash
# Clone the repository
git clone https://github.com/giosilvi/flappy_ai.git
cd flappy_ai

# Install dependencies
cd web_client
npm install

# Start development server
npm run dev
```

Open [http://localhost:5173](http://localhost:5173) in your browser.

### Build for Production

```bash
cd web_client
npm run build
```

The built files will be in `web_client/dist/`.

---

## ğŸ§  How It Works

### DQN (Deep Q-Network)

The agent uses a Deep Q-Network to learn which action (flap or don't flap) maximizes future rewards:

1. **Observation Space** (6 inputs by default):
   - `birdY` â€” Bird's vertical position (normalized)
   - `birdVel` â€” Bird's vertical velocity (normalized)
   - `dx1` â€” Horizontal distance to next pipe
   - `dy1` â€” Vertical distance to next pipe's gap center
   - `dx2` â€” Horizontal distance to second pipe
   - `dy2` â€” Vertical distance to second pipe's gap center

2. **Neural Network Architecture**:
   - Input layer: 6 neurons
   - Hidden layers: 2 Ã— 64 neurons (ReLU activation)
   - Output layer: 2 neurons (Q-values for idle/flap)

3. **Training Process**:
   - Agent takes actions using Îµ-greedy policy
   - Experiences stored in replay buffer (50,000 capacity)
   - Network trained on random mini-batches (32 samples)
   - Target network updated every 200 steps for stability

### Reward Structure

| Event | Default Reward |
|-------|----------------|
| Pass a pipe | +1.0 |
| Each step alive | -0.01 |
| Death (collision) | -1.0 |
| Flap cost | -0.003 |
| Out of bounds (above screen) | -0.005 |
| Center bonus (moving toward gap) | +0.01 |

All rewards are adjustable via the UI during training.

---

## âš™ï¸ Configuration

### Hyperparameters

| Parameter | Default | Range | Description |
|-----------|---------|-------|-------------|
| Epsilon (Îµ) | 0.5 â†’ 0.05 | 0 - 1 | Exploration rate (auto-decays over 150K steps) |
| Learning Rate | 0.001 | 0.0001 - 0.01 | Neural network learning rate |
| Gamma (Î³) | 0.99 | Fixed | Discount factor for future rewards |
| Batch Size | 32 | Fixed | Samples per training step |
| Buffer Size | 50,000 | Fixed | Replay buffer capacity |
| Target Update | Every 200 steps | Fixed | Target network sync frequency |

### Training Modes

| Mode | Description |
|------|-------------|
| **Normal Training** | Full rendering at 30 FPS with live visualization |
| **Fast Training** | Rendering disabled, runs as fast as CPU allows |
| **Evaluation** | Greedy policy (Îµ=0), no training, 30 FPS |
| **Manual Play** | Human control, experiences still added to replay buffer |

---

## ğŸŒ Deployment

### Deploy to a VPS (Hetzner, DigitalOcean, etc.)

```bash
# SSH into your server
ssh root@your-server-ip

# Clone the repository
git clone https://github.com/giosilvi/flappy_ai.git
cd flappy_ai

# Run the deploy script with your domain
chmod +x deploy.sh
./deploy.sh yourdomain.com
```

The deploy script will:
1. Install Node.js 20.x
2. Build the Vue app
3. Set up Caddy with automatic HTTPS
4. Start the containerized web server

### Requirements

- Ubuntu 22.04+ (or similar)
- Docker and Docker Compose
- Domain pointing to your server's IP

---

## ğŸ› ï¸ Tech Stack

### Frontend (`web_client/`)
- **Vue 3** (Option API) â€” Reactive UI framework
- **TypeScript** â€” Type-safe JavaScript
- **Vite** â€” Fast build tool and dev server
- **Canvas API** â€” Game rendering at 30 FPS
- **Web Workers** â€” Background training (fast mode)
- **Custom Neural Network** â€” Pure JS/TS implementation with ReLU activation

### Infrastructure
- **Caddy** â€” Web server with automatic HTTPS
- **Docker** â€” Containerization

### Python Reference (`FlapPyBird/`)
- **PyTorch** â€” Original DQN training
- **Pygame** â€” Original game engine

---

## ğŸ“Š Training Tips

1. **Let auto-decay handle epsilon** â€” Starts at 0.5 and decays to 0.05 over 150K steps
2. **Watch the Q-values** â€” They should stabilize and separate as training progresses
3. **Use fast mode** â€” Training is significantly faster without rendering
4. **Tune rewards carefully** â€” Higher pass pipe reward encourages aggressive play
5. **Save checkpoints** â€” Use the save button to preserve good models
6. **Try manual play** â€” Your gameplay adds to the replay buffer and can help bootstrap learning

---

## ğŸ¯ Roadmap

- [x] Browser-based DQN training
- [x] Real-time neural network visualization with ReLU display
- [x] Hyperparameter tuning UI
- [x] Fast training mode with Web Workers
- [x] Checkpoint save/load
- [x] Leaderboard system
- [ ] Champion model showcase on landing page
- [ ] Mobile-optimized controls
- [ ] Additional RL algorithms (Double DQN, Dueling DQN)

---

## ğŸ“œ License

This project is open source under the MIT License.

---

## ğŸ™ Acknowledgments

- Original Flappy Bird game by Dong Nguyen
- [FlapPyBird](https://github.com/sourabhv/FlapPyBird) â€” Python implementation reference
- Sprites and sounds from the classic Flappy Bird

---

**Built with â¤ï¸ for the RL community**
